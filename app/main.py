# app/main.py
"""
FastAPI Application - Universal Pipeline Edition
Migrated from old architecture to Universal Request Processing
Permanently handles missing old services with proper fallbacks
"""

import asyncio
import logging
import os
import sys
import time
from contextlib import asynccontextmanager

from dotenv import load_dotenv
from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse

# Load environment variables
load_dotenv()

# Add the project root to the Python path
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

# Configure logging
logging.basicConfig(level=logging.INFO, format="%(levelname)s:%(name)s:%(message)s")

# Keep debug for specific services
logging.getLogger("app.services.local_llm_normalizer").setLevel(logging.DEBUG)

logger = logging.getLogger(__name__)

# Global variables for services
enhanced_normalizer = None
dynamic_discovery = None


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan management - Universal Pipeline with proper fallbacks"""
    global enhanced_normalizer, dynamic_discovery

    # Startup
    logger.info("üöÄ Starting NetOps ChatBot API with Universal Pipeline...")

    try:
        # Step 1: Try to initialize enhanced_output_normalizer (may not exist)
        try:
            from app.services.enhanced_output_normalizer import (
                enhanced_normalizer as en,
            )

            enhanced_normalizer = en

            llm_available = await enhanced_normalizer.initialize()
            if llm_available:
                logger.info("‚úÖ Enhanced Output Normalizer with LLM initialized")
            else:
                logger.warning("‚ö†Ô∏è Enhanced Output Normalizer initialized without LLM")
        except ImportError:
            # Create fallback enhanced_normalizer
            logger.warning("‚ö†Ô∏è Enhanced Output Normalizer not found - creating fallback")
            enhanced_normalizer = create_fallback_normalizer()

        # Step 2: Initialize Dynamic Command Discovery (preserve your working setup)
        try:
            from app.services.dynamic_command_discovery import DynamicCommandDiscovery
            from app.services.local_llm_normalizer import local_llm_normalizer

            # Initialize dynamic command discovery with LLM normalizer
            dynamic_discovery = DynamicCommandDiscovery(
                llm_normalizer=local_llm_normalizer,
                ollama_url=os.getenv("OLLAMA_URL", "http://192.168.1.11:1234"),
                model_name=os.getenv("OLLAMA_MODEL", "meta-llama-3.1-8b-instruct"),
                timeout=int(os.getenv("LLM_TIMEOUT", "30")),
            )

            # Set the global instance for import by other modules
            import app.services.dynamic_command_discovery as discovery_module

            discovery_module.dynamic_discovery = dynamic_discovery

            logger.info("üß† Dynamic Command Discovery initialized successfully")
        except Exception as e:
            logger.error(f"‚ùå Dynamic Command Discovery failed to initialize: {e}")

        # Step 3: Initialize Universal Pipeline components
        await initialize_universal_pipeline()

        # Step 4: Start Chat Interface using Universal Pipeline
        await start_universal_chat_interface()

        logger.info("üéØ NetOps ChatBot API ready with Universal Pipeline!")

    except Exception as e:
        logger.error(f"‚ùå Startup failed: {e}")
        raise

    yield

    # Shutdown
    logger.info("üîÑ Shutting down NetOps ChatBot API...")
    await shutdown_services()


def create_fallback_normalizer():
    """Create fallback normalizer when enhanced_output_normalizer doesn't exist"""

    class FallbackNormalizer:
        def __init__(self):
            self.llm_available = False

        async def initialize(self):
            logger.info("‚úÖ Fallback normalizer initialized")
            return False

        def get_stats(self):
            return {
                "status": "fallback",
                "llm_available": False,
                "note": "Using fallback - enhanced_output_normalizer not available",
            }

        def clear_cache(self):
            return 0

    return FallbackNormalizer()


async def initialize_universal_pipeline():
    """Initialize Universal Pipeline components"""

    # Initialize Universal Request Processor
    try:
        from app.services.universal_request_processor import universal_processor

        if await universal_processor.initialize():
            logger.info("‚úÖ Universal Request Processor initialized")
        else:
            logger.warning("‚ö†Ô∏è Universal Request Processor initialization had issues")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Universal Request Processor error: {e}")

    # Initialize Universal Formatter
    try:
        from app.services.universal_formatter import universal_formatter

        if await universal_formatter.initialize():
            logger.info("‚úÖ Universal Formatter initialized with LLM support")
        else:
            logger.warning("‚ö†Ô∏è Universal Formatter initialized without LLM support")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Universal Formatter error: {e}")

    # Initialize Intent Parser
    try:
        logger.info("‚úÖ Intent Parser initialized")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Intent Parser error: {e}")


async def start_universal_chat_interface():
    """Start chat interface using Universal Pipeline"""

    try:
        from app.interfaces.chat_adapter import chat_adapter

        # Start chat adapter in background
        asyncio.create_task(chat_adapter.start_listening())
        logger.info("‚úÖ Universal Chat Interface started - listening for messages")

    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Universal Chat Interface not started: {e}")


async def shutdown_services():
    """Shutdown all services gracefully"""

    try:
        # Stop Universal chat interface
        from app.interfaces.chat_adapter import chat_adapter

        await chat_adapter.stop_listening()
        logger.info("‚úÖ Universal Chat Interface stopped")

    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Shutdown issue: {e}")


# Create FastAPI application
app = FastAPI(
    title="NetOps ChatBot API - Universal Pipeline",
    description="""
    üöÄ **Universal AI-Driven Network Automation Platform**

    Revolutionary universal pipeline that processes ANY network automation request
    from ANY interface (Chat, API, CLI, Web) using AI-powered command discovery.

    ## üåü Key Features

    - **Universal Interface**: One pipeline handles Chat, API, CLI, Web requests
    - **AI Command Discovery**: Automatically discovers commands for any vendor/platform
    - **Intelligent Formatting**: LLM formats responses for any output format
    - **Multi-Vendor Support**: Works with Cisco, Arista, Juniper, and any network vendor
    - **Self-Learning**: System gets faster over time through intelligent caching

    ## üéØ Main Endpoints

    - `/api/v1/universal` - Universal endpoint for any request
    - `/api/v1/chat` - Chat interface simulation
    - `/api/v1/query` - Quick URL-based queries
    - `/docs` - Interactive API documentation

    ## ü§ñ Chat Interface

    Use natural language via Mattermost:
    - `@netops-bot show version on spine1`
    - `@netops-bot show serial number on spine1` (AI discovers this!)
    """,
    version="2.0.0-universal",
    lifespan=lifespan,
)

# CORS middleware (preserve your existing config)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# Request timing middleware
@app.middleware("http")
async def add_process_time_header(request: Request, call_next):
    start_time = time.time()
    response = await call_next(request)
    process_time = time.time() - start_time
    response.headers["X-Process-Time"] = str(process_time)
    return response


# Global exception handler
@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    logger.error(f"Global exception: {exc}")
    return JSONResponse(
        status_code=500,
        content={
            "error": "Internal server error",
            "detail": str(exc),
            "path": str(request.url),
        },
    )


# Include routers with proper error handling and fallbacks

# NEW: Universal Router (this is the main one we want)
try:
    from app.routers.universal import router as universal_router

    app.include_router(universal_router)
    logger.info("‚úÖ Successfully included Universal router")
except Exception as e:
    logger.error(f"‚ùå Failed to include Universal router: {e}")

# Keep working routers only - with safe imports
try:
    from app.routers import discovery

    app.include_router(discovery.router, prefix="/api/v1/discovery", tags=["discovery"])
    logger.info("‚úÖ Successfully included discovery router")
except Exception as e:
    logger.error(f"‚ùå Failed to include discovery router: {e}")

# Skip problematic routers for now - they need to be updated for Universal Pipeline
# These routers import old services that no longer exist
logger.info(
    "‚ÑπÔ∏è Skipping legacy routers (devices, test_llm, test_mattermost) - use Universal Pipeline endpoints instead"
)

# Note: Users can now use:
# - /api/v1/universal for all network operations
# - /api/v1/chat for chat simulation
# - /api/v1/query for quick URL-based queries


# Root endpoint
@app.get("/")
async def root():
    """Root endpoint with system information"""

    try:
        # Try to get Universal Pipeline status
        from app.services.universal_request_processor import universal_processor

        health = await universal_processor.health_check()
        stats = universal_processor.get_stats()

        return {
            "message": "NetOps ChatBot API - Universal Pipeline Active",
            "version": "2.0.0-universal",
            "status": "healthy"
            if health.get("universal_processor") == "healthy"
            else "degraded",
            "features": [
                "Universal AI-powered automation",
                "Dynamic command discovery",
                "Multi-interface support (Chat/API/CLI/Web)",
                "Intelligent response formatting",
                "Self-learning command mapping",
            ],
            "statistics": stats,
            "endpoints": {
                "universal": "/api/v1/universal",
                "chat_simulation": "/api/v1/chat",
                "quick_query": "/api/v1/query",
                "documentation": "/docs",
            },
        }

    except Exception as e:
        # Fallback if Universal Pipeline not ready
        return {
            "message": "NetOps ChatBot API - Universal Pipeline",
            "version": "2.0.0-universal",
            "status": "initializing",
            "error": str(e),
            "documentation": "/docs",
        }


# Enhanced health check with proper fallback handling
@app.get("/health")
async def health_check():
    """Enhanced health check with Universal Pipeline status and fallbacks"""

    health_status = {
        "status": "healthy",
        "service": "netops-chatbot-universal",
        "timestamp": time.time(),
    }

    try:
        # Check enhanced normalizer (with fallback support)
        if enhanced_normalizer:
            health_status["llm_available"] = getattr(
                enhanced_normalizer, "llm_available", False
            )
        else:
            health_status["llm_available"] = False

        # Check dynamic discovery (your existing working component)
        if dynamic_discovery:
            health_status["dynamic_discovery_available"] = True
            health_status["discovery_stats"] = dynamic_discovery.get_discovery_stats()
        else:
            health_status["dynamic_discovery_available"] = False

        # Check Universal Pipeline
        try:
            from app.services.universal_request_processor import universal_processor

            universal_health = await universal_processor.health_check()
            health_status["universal_pipeline"] = universal_health
        except Exception as e:
            health_status["universal_pipeline"] = f"error: {str(e)}"

        # Check Chat Interface
        try:
            from app.interfaces.chat_adapter import chat_adapter

            chat_health = await chat_adapter.health_check()
            health_status["chat_interface"] = chat_health
        except Exception as e:
            health_status["chat_interface"] = f"error: {str(e)}"

    except Exception as e:
        health_status["status"] = "degraded"
        health_status["error"] = str(e)

    return health_status


# Keep discovery endpoints with proper fallback handling
@app.get("/api/v1/discovery/stats", tags=["discovery"])
async def get_discovery_stats():
    """Get dynamic discovery statistics with fallback"""
    try:
        if not dynamic_discovery:
            return {
                "error": "Dynamic discovery not available",
                "status": "not_initialized",
            }

        discovery_stats = dynamic_discovery.get_discovery_stats()
        cache_stats = dynamic_discovery.get_cache_stats()

        return {
            "discovery_statistics": discovery_stats,
            "cache_statistics": cache_stats,
            "service_status": "active",
        }
    except Exception as e:
        return {"error": str(e), "status": "error"}


# Keep LLM endpoints with proper fallback handling
@app.get("/api/v1/llm/status", tags=["llm-testing"])
async def llm_status():
    """Get Local LLM status with fallback support"""
    try:
        if not enhanced_normalizer:
            return {
                "error": "Enhanced normalizer not available",
                "llm_available": False,
                "service": "netops-chatbot-llm-fallback",
            }

        return {
            "llm_available": getattr(enhanced_normalizer, "llm_available", False),
            "normalization_stats": enhanced_normalizer.get_stats(),
            "service": "netops-chatbot-llm",
        }
    except Exception as e:
        return {"error": str(e), "llm_available": False}


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(
        "app.main:app", host="0.0.0.0", port=8000, reload=True, log_level="info"
    )
